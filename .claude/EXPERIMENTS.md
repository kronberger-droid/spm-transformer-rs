# Training Experiments Log

## Experiment Naming Convention

`YYYYMMDD_HHMMSS` or `YYYYMMDD_HHMMSS_jSLURM_JOB_ID`

---

## Baseline Experiments (Before Fixes)

### Experiment: Rust ViT Baseline (Pre-fixes)
**Date**: 2025-12-01 (approximate)
**Location**: `checkpoints/` (old runs)

**Configuration**:
- Model: ScanLineEncoder (ViT)
- Layers: 4
- d_model: 256
- Heads: 8
- LR: 1e-3 (constant, no scheduling)
- Batch size: 32
- Epochs: 50
- Class weights: ‚ùå NOT used
- Normalization: ‚ùå None
- Num classes: 6 ‚ùå (should be 4)

**Results**:
```
Train accuracy: ~36% (epoch 50)
Valid accuracy: ~36% (epoch 50)
Train loss: ~1.60
Valid loss: ~1.60
```

**Analysis**:
- No overfitting (train ‚âà valid)
- Model appears stuck/plateaued
- Close to random guessing for 4 classes (25%)
- Multiple critical issues identified

**Issues**:
1. Class weights not used ‚Üí imbalanced learning
2. No normalization ‚Üí unstable gradients
3. LR too high ‚Üí overshooting
4. Wrong num_classes ‚Üí incorrect output layer
5. Only 4 layers ‚Üí less capacity than Python (6 layers)

---

### Experiment: Python ViT Reference
**Date**: 2025-11 (approximate)
**Location**: `~/Programming/python/transformer-line-classification/`

**Configuration**:
- Model: Vision Transformer
- Layers: 6
- d_model: 256
- Heads: 8
- FFN dims: 1024
- LR: 1e-4 (warmup + ReduceLROnPlateau)
- Batch size: 32
- Epochs: 54 (early stopping)
- Class weights: ‚úÖ [2.845, 0.569, 1.478, 0.823]
- Normalization: ‚úÖ Per-scanline (mean=0, std=1)
- Augmentation: ‚úÖ Horizontal flip, Gaussian noise, line dropout

**Results**:
```
Best validation accuracy: 39.23% (epoch 39)
Best validation AUROC: 0.6633
Training stopped: epoch 54 (early stopping)
```

**Analysis**:
- Still significantly underperforms target (39% vs 85%)
- Proper training setup but wrong architecture paradigm
- ViT approach may not capture sequential nature of STM scanning

---

## Experiments with Fixes (Phase 1)

### Experiment: Run 1 - All 5 fixes applied üéâ
**Date**: 2025-12-03
**Status**: ‚úÖ Complete

**Configuration**:
- Model: ScanLineEncoder (ViT - global attention, no causal masking)
- Layers: 6 ‚úÖ
- d_model: 256
- Heads: 8
- LR: 1e-4 ‚úÖ
- LR Schedule: ‚úÖ Linear warmup (1e-6 ‚Üí 1e-4 over 5 epochs)
- Batch size: 32
- Epochs: 50
- Dropout: 0.1
- Class weights: ‚úÖ [2.845, 0.569, 1.478, 0.823]
- Normalization: ‚úÖ Per-scanline (mean=0, std=1)
- Num classes: ‚úÖ 4 (auto-detected)
- Weight decay: ‚ùå None (not yet added)

**Results**:
```
=== train directory ===
Total epochs: 50
Best accuracy: 83.1928 (epoch 50)
Best loss: 0.357031 (epoch 50)

=== valid directory ===
Total epochs: 50
Best accuracy: 66.4982 (epoch 49)
Best loss: 1.237079 (epoch 27) ‚Üê Validation loss increased after this!
Latest accuracy: 66.1257 (epoch 50)
Latest loss: 1.443985 (epoch 50)
```

**Analysis**:
- ‚úÖ **HUGE SUCCESS!** Validation: 66.5% (vs baseline 36%)
- ‚úÖ **+30 percentage points improvement** from all fixes
- ‚úÖ **Beats Python ViT by 27 points** (66.5% vs 39%)
- ‚ö†Ô∏è **Overfitting**: 17-point train/valid gap (83% vs 66%)
- ‚ö†Ô∏è **Validation loss degraded** after epoch 27 (1.237 ‚Üí 1.444)
- üìä Training still improving at epoch 50 (not saturated)

**Key Insights**:
1. All 5 critical fixes worked perfectly
2. Model has capacity to learn (83% train accuracy)
3. **Needs regularization** - overfitting is the bottleneck
4. Optimal stopping point: epoch 27-30
5. Still ViT architecture (not sequential transformer yet)

**Next Steps**:
1. Add weight decay (try 0.01, 0.05)
2. Increase dropout (try 0.15, 0.2)
3. Add early stopping
4. Target: 70-73% validation accuracy

---

## Phase 2: Hyperparameter Sweep (Regularization)

### Experiment: 8-Config Hyperparameter Sweep
**Date**: 2025-12-03
**Job ID**: 324891
**Status**: ‚úÖ Complete

**Sweep Configuration**:
- Base model: ScanLineEncoder (ViT - 6 layers, d_model=256, heads=8)
- LR: 1e-4 (5-epoch warmup)
- Batch size: 32
- Epochs: 50
- **Grid Variables**:
  - Weight decay: [0.0, 0.01, 0.05, 0.1]
  - Dropout: [0.1, 0.15, 0.2]
- Total configs: 8

**Results Table**:

| Config | WD | Drop | Best Valid Acc | Best Valid Loss (Epoch) | Train Acc | Train/Valid Gap | Best Epoch |
|--------|-----|------|----------------|------------------------|-----------|----------------|------------|
| no_wd_control | 0.0 | 0.1 | **66.74%** | 1.254 (20) | 84.35% | 18.99 | 48 |
| baseline | 0.01 | 0.1 | 66.06% | 1.242 (19) | 83.80% | 18.14 | 44 |
| wd05_drop10 | 0.05 | 0.1 | 65.59% | 1.209 (24) | 83.70% | 18.42 | 48 |
| wd10_drop15 | 0.1 | 0.15 | 65.07% | 1.240 (30) | 76.92% | 14.21 | 47 |
| wd01_drop15 | 0.01 | 0.15 | 64.86% | 1.241 (27) | 77.55% | 14.91 | 47 |
| wd05_drop15 | 0.05 | 0.15 | 64.51% | 1.203 (27) | 79.03% | 14.52 | 50 |
| wd01_drop20 | 0.01 | 0.2 | 63.25% | 1.277 (34) | 73.59% | 11.52 | 49 |
| wd05_drop20 | 0.05 | 0.2 | 60.54% | 1.256 (32) | 73.02% | 13.66 | 46 |

**Detailed Results**:

#### Config 1: no_wd_control (BEST)
```
Weight Decay: 0.0, Dropout: 0.1
Valid: 66.74% acc, 1.254 loss (epoch 20)
Train: 84.35% acc, 0.338 loss (epoch 50)
Gap: 18.99 points
```

#### Config 2: baseline
```
Weight Decay: 0.01, Dropout: 0.1
Valid: 66.06% acc, 1.242 loss (epoch 19)
Train: 83.80% acc, 0.349 loss (epoch 50)
Gap: 18.14 points
```

#### Config 3: wd05_drop10
```
Weight Decay: 0.05, Dropout: 0.1
Valid: 65.59% acc, 1.209 loss (epoch 24)
Train: 83.70% acc, 0.351 loss (epoch 50)
Gap: 18.42 points
```

#### Config 4: wd10_drop15
```
Weight Decay: 0.1, Dropout: 0.15
Valid: 65.07% acc, 1.240 loss (epoch 30)
Train: 76.92% acc, 0.506 loss (epoch 50)
Gap: 14.21 points
```

#### Config 5: wd01_drop15
```
Weight Decay: 0.01, Dropout: 0.15
Valid: 64.86% acc, 1.241 loss (epoch 27)
Train: 77.55% acc, 0.480 loss (epoch 49)
Gap: 14.91 points
```

#### Config 6: wd05_drop15
```
Weight Decay: 0.05, Dropout: 0.15
Valid: 64.51% acc, 1.203 loss (epoch 27)
Train: 79.03% acc, 0.457 loss (epoch 50)
Gap: 14.52 points
```

#### Config 7: wd01_drop20
```
Weight Decay: 0.01, Dropout: 0.2
Valid: 63.25% acc, 1.277 loss (epoch 34)
Train: 73.59% acc, 0.574 loss (epoch 50)
Gap: 11.52 points
```

#### Config 8: wd05_drop20
```
Weight Decay: 0.05, Dropout: 0.2
Valid: 60.54% acc, 1.256 loss (epoch 32)
Train: 73.02% acc, 0.585 loss (epoch 49)
Gap: 13.66 points
```

**Analysis**:

**Key Findings**:
1. **ViT Architecture Ceiling**: ~66-67% validation accuracy maximum
   - All configs cluster in 60-67% range
   - Best config (no regularization): 66.74%
   - Target (Gordon et al. 2020): 85% - **19 points short!**

2. **Regularization Trade-off**:
   - Stronger regularization ‚Üí Lower train/valid gap BUT also lower validation accuracy
   - No weight decay performs BEST (66.74%)
   - Heavy regularization (wd=0.05, drop=0.2) ‚Üí only 60.54%
   - **Regularization reduces model capacity more than it helps overfitting**

3. **Validation Degradation Pattern Persists**:
   - All configs: best loss at epoch 20-34, then degrades by epoch 50
   - Example: no_wd best loss @ epoch 20, latest loss 1.570 @ epoch 50
   - Early stopping would help compute but not accuracy ceiling

4. **Architecture is the Bottleneck**:
   - Even with minimal overfitting (11-point gap), validation only reaches 63%
   - Global attention ViT cannot capture sequential nature of STM scans
   - Need sequential transformer with causal masking

**Conclusions**:
- ‚úÖ Phase 2 complete: ViT optimized to maximum capability
- ‚ùå ViT cannot reach 85% target (ceiling at ~67%)
- ‚û°Ô∏è **Move to Phase 3**: Sequential Transformer Architecture required
- **Best config for future ViT experiments**: no weight decay, dropout=0.1

**Next Steps**:
1. Implement sequential transformer with causal masking
2. Add CNN tokenizer per scanline
3. Target: 85%+ validation accuracy

---

## Experiment Template

### Experiment: [Name]
**Date**: YYYY-MM-DD
**Checkpoint**: `checkpoints/[id]/`
**Status**: [Pending / Running / Complete]

**Configuration**:
- Model:
- Layers:
- d_model:
- Heads:
- LR:
- LR Schedule:
- Batch size:
- Epochs:
- Class weights:
- Normalization:
- Other:

**Results**:
```
[Paste analyzer output or key metrics]
```

**Analysis**:
- [What worked well]
- [What didn't work]
- [Unexpected findings]

**Next Steps**:
- [Planned follow-up experiments]

---

## Ablation Studies (Future)

### Planned Ablations

1. **Class Weights Impact**
   - Run with and without class weights
   - Expected: Significant impact on minority classes

2. **Normalization Impact**
   - Run with and without per-scanline normalization
   - Expected: Affects training stability

3. **Learning Rate**
   - Test: 1e-5, 1e-4, 5e-4, 1e-3
   - Expected: 1e-4 optimal

4. **Number of Layers**
   - Test: 4, 6, 8, 12
   - Expected: Diminishing returns after 6-8

5. **Model Size**
   - Test d_model: 128, 256, 512
   - Expected: 256 optimal for dataset size

---

## Notes

- Always use `cargo run --release --bin analyze -- --checkpoints-path checkpoints/[id]/` to analyze results
- Compare train vs valid metrics to detect overfitting
- Track best epoch, not just final epoch
- Save config.json with each checkpoint for reproducibility
